{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ca22ce-e962-4aa7-aa0d-9377f965413c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "setInterval(function(){ console.log(\"keeping alive\"); }, 300000);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep Binder session alive during workshop\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def workshop_keepalive():\n",
    "    \"\"\"Keep session alive during workshop presentations\"\"\"\n",
    "    count = 0\n",
    "    while count < 200:  # Run for ~16 hours max\n",
    "        time.sleep(300)  # 5 minutes\n",
    "        count += 1\n",
    "        clear_output(wait=True)\n",
    "        print(f\" Workshop session active - {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\" Runtime: {count * 5} minutes\")\n",
    "        print(\"Continue with the workshop content below...\")\n",
    "\n",
    "# Start in background\n",
    "threading.Thread(target=workshop_keepalive, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b720b-94f2-4375-b396-37a7a37e5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIF360 Bias Detection & Mitigation Workshop\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# AIF360 imports\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae62dbf-6c5f-43e8-b52f-00cc3b9b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import advanced features (may not be available in all environments)\n",
    "try:\n",
    "    from aif360.explainers import MetricTextExplainer\n",
    "    EXPLAINER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\" MetricTextExplainer not available in this environment\")\n",
    "    EXPLAINER_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import aif360.sklearn.metrics as aif_sklearn_metrics\n",
    "    SKLEARN_METRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_METRICS_AVAILABLE = False\n",
    "\n",
    "print(\" Welcome to the AIF360 Example!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e2b8c-504d-4e7b-80e8-b90ccbcc89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: BUSINESS CONTEXT - INCOME PREDICTION SCENARIOS\n",
    "# =============================================================================\n",
    "print(\"\\n BUSINESS CONTEXT FOR INCOME PREDICTION MODELS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "REAL-WORLD APPLICATIONS WHERE THIS MATTERS:\n",
    "• Credit scoring and loan approvals\n",
    "• Insurance premium calculations\n",
    "• Marketing segmentation for high-value products\n",
    "• Economic research and policy analysis\n",
    "• Financial planning and advisory services\n",
    "\n",
    "KEY BUSINESS QUESTIONS:\n",
    "• Are our income prediction models systematically biased against women?\n",
    "• What are the legal/regulatory risks of gender-biased predictions?\n",
    "• How do we balance model accuracy with fairness requirements?\n",
    "• What tools can help us measure and mitigate algorithmic bias?\n",
    "\n",
    "This analysis demonstrates AIF360's capabilities using census data to predict\n",
    "whether individuals earn more than $50,000 annually, while ensuring gender fairness.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf74e49-695a-4c49-8f89-b1b351fef414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "print(\"\\n DATA LOADING AND PREPARATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Define column names for Adult dataset\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "               'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "               'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Load the data files from your repo\n",
    "print(\" Loading Adult Census dataset from local files...\")\n",
    "df_train = pd.read_csv('data/adult/adult.data', names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "df_test = pd.read_csv('data/adult/adult.test', names=column_names, na_values=' ?', skipinitialspace=True, skiprows=1)\n",
    "\n",
    "# Combine datasets\n",
    "df_raw = pd.concat([df_train, df_test], ignore_index=True)\n",
    "print(f\" Loaded {len(df_raw):,} records\")\n",
    "\n",
    "# Clean the data\n",
    "print(\" Cleaning and preparing data...\")\n",
    "# Remove missing values\n",
    "df_clean = df_raw.dropna()\n",
    "print(f\"After removing missing values: {len(df_clean):,} records\")\n",
    "\n",
    "# Clean income column (remove periods from test set)\n",
    "df_clean['income'] = df_clean['income'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Create binary target variable (0: <=50K, 1: >50K)\n",
    "df_clean['high_income'] = (df_clean['income'] == '>50K').astype(int)\n",
    "\n",
    "# Create binary sex variable (0: Female, 1: Male)\n",
    "df_clean['sex_binary'] = (df_clean['sex'] == 'Male').astype(int)\n",
    "\n",
    "# Encode categorical variables for AIF360 compatibility\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                   'relationship', 'race', 'native-country']\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_processed = df_clean.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "\n",
    "# Create the final cleaned dataset\n",
    "df_final = df_processed[['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                        'marital-status', 'occupation', 'relationship', 'race', \n",
    "                        'sex_binary', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                        'native-country', 'high_income']].copy()\n",
    "\n",
    "print(\" Data preparation complete!\")\n",
    "print(f\"Final dataset: {df_final.shape[0]} rows × {df_final.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n Dataset Demographics:\")\n",
    "print(f\"Female (0): {(df_final['sex_binary'] == 0).sum():,} ({(df_final['sex_binary'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Male (1): {(df_final['sex_binary'] == 1).sum():,} ({(df_final['sex_binary'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"High income (>$50K): {df_final['high_income'].sum():,} ({df_final['high_income'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Show income distribution by gender\n",
    "income_by_gender = df_final.groupby(['sex_binary', 'high_income']).size().unstack(fill_value=0)\n",
    "income_rates = df_final.groupby('sex_binary')['high_income'].mean()\n",
    "\n",
    "print(\"\\n Income Distribution by Gender:\")\n",
    "print(f\"Female high-income rate: {income_rates[0]:.1%}\")\n",
    "print(f\"Male high-income rate: {income_rates[1]:.1%}\")\n",
    "print(f\"Raw gender gap: {income_rates[1] - income_rates[0]:.1%} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f237784-4e3a-4e47-a92f-7547347750a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: CREATE AIF360 DATASET AND INITIAL BIAS ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n SECTION 3: INITIAL BIAS MEASUREMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create AIF360 StandardDataset\n",
    "dataset = StandardDataset(\n",
    "    df=df_final,\n",
    "    label_name='high_income',\n",
    "    favorable_classes=[1],  # High income is the favorable outcome\n",
    "    protected_attribute_names=['sex_binary'],\n",
    "    privileged_classes=[[1]],  # Male = 1 (historically privileged in income)\n",
    "    categorical_features=['workclass', 'education', 'marital-status', 'occupation', \n",
    "                         'relationship', 'race', 'native-country']\n",
    ")\n",
    "\n",
    "print(\" Created AIF360 dataset structure\")\n",
    "\n",
    "# Split the data\n",
    "train_dataset, test_dataset = dataset.split([0.7], shuffle=True, seed=123)\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "privileged_groups = [{'sex_binary': 1}]    # Male\n",
    "unprivileged_groups = [{'sex_binary': 0}]  # Female\n",
    "\n",
    "# Calculate bias metrics on training data\n",
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "    train_dataset,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"\\n BIAS METRICS IN ORIGINAL TRAINING DATA:\")\n",
    "disparate_impact = metric_orig_train.disparate_impact()\n",
    "statistical_parity_diff = metric_orig_train.statistical_parity_difference()\n",
    "\n",
    "print(f\"Disparate Impact: {disparate_impact:.3f}\")\n",
    "print(f\"Statistical Parity Difference: {statistical_parity_diff:.3f}\")\n",
    "\n",
    "print(\"\\n INTERPRETATION FOR STAKEHOLDERS:\")\n",
    "print(\"• Disparate Impact < 0.8 = Significant gender bias in income prediction\")\n",
    "print(\"• Statistical Parity Difference > 0.1 = Substantial disparity between groups\")\n",
    "print(\"• These metrics help assess fairness in algorithmic decision-making\")\n",
    "\n",
    "# Risk assessment for income prediction context\n",
    "risk_level = \" HIGH BIAS\" if disparate_impact < 0.8 else \" MODERATE BIAS\" if disparate_impact < 0.9 else \" LOW BIAS\"\n",
    "print(f\"• Current Bias Level: {risk_level}\")\n",
    "\n",
    "if disparate_impact < 0.8:\n",
    "    print(\"  → Model systematically underestimates women's income potential\")\n",
    "elif disparate_impact < 0.9:\n",
    "    print(\"  → Model shows measurable bias that should be addressed\")\n",
    "else:\n",
    "    print(\"  → Model shows acceptable gender parity in predictions\")\n",
    "\n",
    "# Visualize the bias\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Get training data as DataFrame for visualization\n",
    "train_df = train_dataset.convert_to_dataframe()[0]\n",
    "\n",
    "# High income rates by gender\n",
    "outcome_by_gender = train_df.groupby('sex_binary')['high_income'].mean()\n",
    "bars = ax1.bar(['Female', 'Male'], outcome_by_gender, color=['#ff7f7f', '#7f7fff'])\n",
    "ax1.set_title('High-Income Rate by Gender\\n(Training Data)')\n",
    "ax1.set_ylabel('Rate of High-Income (>$50K)')\n",
    "ax1.set_ylim(0, 0.5)\n",
    "\n",
    "for i, v in enumerate(outcome_by_gender):\n",
    "    ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Add gap annotation\n",
    "gap = outcome_by_gender[1] - outcome_by_gender[0]\n",
    "ax1.text(0.5, max(outcome_by_gender) + 0.05, f'Gap: {gap:.1%}', \n",
    "         ha='center', fontsize=12, fontweight='bold', \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "# Bias dashboard\n",
    "color = '#ff4444' if disparate_impact < 0.8 else '#ffaa00' if disparate_impact < 0.9 else '#44ff44'\n",
    "ax2.text(0.5, 0.7, f'Disparate Impact: {disparate_impact:.3f}', \n",
    "         ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax2.text(0.5, 0.5, risk_level, ha='center', va='center', fontsize=16, \n",
    "         fontweight='bold', color=color)\n",
    "ax2.text(0.5, 0.3, 'Fairness threshold: 0.8', ha='center', va='center', fontsize=12)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('Gender Bias Assessment')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39fd60-20be-4c56-9a18-bc1067ee318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: AIF360 BIAS MITIGATION - REWEIGHING\n",
    "# =============================================================================\n",
    "print(\"\\n SECTION 4: AIF360 BIAS MITIGATION\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "print(\"AIF360 offers multiple bias mitigation approaches:\")\n",
    "print(\"• PRE-processing: Adjust training data to reduce bias (Reweighing)\")\n",
    "print(\"• IN-processing: Build fairness constraints into model training\")  \n",
    "print(\"• POST-processing: Calibrate model outputs for fairness\")\n",
    "print(\"\\n Demonstrating PRE-processing with Reweighing algorithm...\")\n",
    "\n",
    "# Apply Reweighing algorithm\n",
    "np.random.seed(123)\n",
    "reweighing = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Transform training data\n",
    "train_dataset_reweighed = reweighing.fit_transform(train_dataset)\n",
    "print(\" Applied Reweighing to training data\")\n",
    "\n",
    "# Check bias metrics after reweighing\n",
    "metric_reweighed = BinaryLabelDatasetMetric(\n",
    "    train_dataset_reweighed,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"\\n BIAS METRICS AFTER REWEIGHING:\")\n",
    "disparate_impact_new = metric_reweighed.disparate_impact()\n",
    "statistical_parity_diff_new = metric_reweighed.statistical_parity_difference()\n",
    "\n",
    "print(f\"Original Disparate Impact: {disparate_impact:.3f}\")\n",
    "print(f\"After Reweighing: {disparate_impact_new:.3f}\")\n",
    "\n",
    "improvement = ((disparate_impact_new - disparate_impact) / abs(disparate_impact) * 100)\n",
    "print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "print(f\"\\nOriginal Statistical Parity Diff: {statistical_parity_diff:.3f}\")\n",
    "print(f\"After Reweighing: {statistical_parity_diff_new:.3f}\")\n",
    "\n",
    "# Show what reweighing actually did\n",
    "print(f\"\\n HOW REWEIGHING WORKS:\")\n",
    "print(\"The algorithm adjusts sample weights to balance representation:\")\n",
    "\n",
    "# Get original and reweighed weights by group\n",
    "train_df = train_dataset.convert_to_dataframe()[0]\n",
    "weights_orig = train_dataset.instance_weights\n",
    "weights_new = train_dataset_reweighed.instance_weights\n",
    "\n",
    "# Calculate average weights by group\n",
    "for sex, label in [(0, 'Female'), (1, 'Male')]:\n",
    "    mask = train_df['sex_binary'] == sex\n",
    "    avg_weight_orig = weights_orig[mask].mean()\n",
    "    avg_weight_new = weights_new[mask].mean()\n",
    "    print(f\"• {label} samples - Original weight: {avg_weight_orig:.3f}, New weight: {avg_weight_new:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49cc81-4412-4dcf-81aa-0f8010446f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: MODEL TRAINING AND PERFORMANCE COMPARISON\n",
    "# =============================================================================\n",
    "print(\"\\n SECTION 5: MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"Training income prediction models to compare performance and fairness...\")\n",
    "\n",
    "# Prepare data for sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Original model (without bias mitigation)\n",
    "X_train_orig = scaler.fit_transform(train_dataset.features)\n",
    "y_train_orig = train_dataset.labels.ravel()\n",
    "model_orig = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)\n",
    "model_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "# Bias-mitigated model (with reweighing)\n",
    "X_train_reweighed = scaler.fit_transform(train_dataset_reweighed.features)\n",
    "y_train_reweighed = train_dataset_reweighed.labels.ravel()\n",
    "sample_weights = train_dataset_reweighed.instance_weights\n",
    "\n",
    "model_reweighed = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)\n",
    "model_reweighed.fit(X_train_reweighed, y_train_reweighed, sample_weight=sample_weights)\n",
    "\n",
    "# Test both models\n",
    "X_test = scaler.transform(test_dataset.features)\n",
    "y_test = test_dataset.labels.ravel()\n",
    "\n",
    "y_pred_orig = model_orig.predict(X_test)\n",
    "y_pred_reweighed = model_reweighed.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_orig = accuracy_score(y_test, y_pred_orig)\n",
    "accuracy_reweighed = accuracy_score(y_test, y_pred_reweighed)\n",
    "\n",
    "print(\"\\n EXECUTIVE DASHBOARD: MODEL COMPARISON\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\" PREDICTIVE PERFORMANCE:\")\n",
    "print(f\"Original Model Accuracy: {accuracy_orig:.1%}\")\n",
    "print(f\"Bias-Mitigated Model Accuracy: {accuracy_reweighed:.1%}\")\n",
    "performance_change = accuracy_reweighed - accuracy_orig\n",
    "print(f\"Performance Trade-off: {performance_change:+.1%}\")\n",
    "\n",
    "if abs(performance_change) < 0.01:\n",
    "    print(\"  → Minimal impact on predictive accuracy\")\n",
    "elif performance_change > 0:\n",
    "    print(\"  → Improved accuracy with bias mitigation!\")\n",
    "else:\n",
    "    print(\"  → Small accuracy reduction for fairness gains\")\n",
    "\n",
    "# Calculate detailed metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_orig = precision_score(y_test, y_pred_orig)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "precision_reweighed = precision_score(y_test, y_pred_reweighed)\n",
    "recall_reweighed = recall_score(y_test, y_pred_reweighed)\n",
    "f1_reweighed = f1_score(y_test, y_pred_reweighed)\n",
    "\n",
    "print(f\"\\n DETAILED PERFORMANCE METRICS:\")\n",
    "print(f\"                     Original    Bias-Mitigated    Change\")\n",
    "print(f\"Precision:           {precision_orig:.3f}       {precision_reweighed:.3f}         {precision_reweighed-precision_orig:+.3f}\")\n",
    "print(f\"Recall:              {recall_orig:.3f}       {recall_reweighed:.3f}         {recall_reweighed-recall_orig:+.3f}\")\n",
    "print(f\"F1-Score:            {f1_orig:.3f}       {f1_reweighed:.3f}         {f1_reweighed-f1_orig:+.3f}\")\n",
    "\n",
    "# Calculate fairness metrics for predictions\n",
    "test_pred_orig = test_dataset.copy()\n",
    "test_pred_orig.labels = y_pred_orig\n",
    "\n",
    "test_pred_reweighed = test_dataset.copy() \n",
    "test_pred_reweighed.labels = y_pred_reweighed\n",
    "\n",
    "# Use ClassificationMetric for comprehensive fairness analysis\n",
    "cm_orig = ClassificationMetric(\n",
    "    test_dataset, test_pred_orig,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "cm_reweighed = ClassificationMetric(\n",
    "    test_dataset, test_pred_reweighed, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"\\n FAIRNESS METRICS ON TEST PREDICTIONS:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "fairness_metrics = [\n",
    "    (\"Disparate Impact\", cm_orig.disparate_impact(), cm_reweighed.disparate_impact(), \">0.8 (fairness threshold)\"),\n",
    "    (\"Statistical Parity Diff\", cm_orig.statistical_parity_difference(), cm_reweighed.statistical_parity_difference(), \"~0.0 (equal rates)\"),\n",
    "    (\"Equal Opportunity Diff\", cm_orig.equal_opportunity_difference(), cm_reweighed.equal_opportunity_difference(), \"~0.0 (equal TPR)\"),\n",
    "    (\"Average Odds Diff\", cm_orig.average_odds_difference(), cm_reweighed.average_odds_difference(), \"~0.0 (balanced errors)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Metric':<25} {'Original':<10} {'Mitigated':<10} {'Target':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for name, orig_val, reweighed_val, target in fairness_metrics:\n",
    "    print(f\"{name:<25} {orig_val:<10.3f} {reweighed_val:<10.3f} {target:<20}\")\n",
    "\n",
    "# Risk levels\n",
    "di_orig = cm_orig.disparate_impact()\n",
    "di_reweighed = cm_reweighed.disparate_impact()\n",
    "\n",
    "orig_risk = \" HIGH\" if di_orig < 0.8 else \" MODERATE\" if di_orig < 0.9 else \" LOW\"\n",
    "reweighed_risk = \" HIGH\" if di_reweighed < 0.8 else \" MODERATE\" if di_reweighed < 0.9 else \" LOW\"\n",
    "\n",
    "print(f\"\\n BIAS RISK ASSESSMENT:\")\n",
    "print(f\"Original Model: {orig_risk}\")  \n",
    "print(f\"Bias-Mitigated Model: {reweighed_risk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd01b2-95b4-4763-9493-9081a7f68cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: COMPREHENSIVE RESULTS VISUALIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n SECTION 6: COMPREHENSIVE RESULTS VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Disparate Impact Comparison\n",
    "models = ['Original', 'Bias-Mitigated']\n",
    "di_values = [di_orig, di_reweighed]\n",
    "colors = ['red' if x < 0.8 else 'orange' if x < 0.9 else 'green' for x in di_values]\n",
    "\n",
    "bars = ax1.bar(models, di_values, color=colors, alpha=0.7)\n",
    "ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Fairness Threshold (0.8)')\n",
    "ax1.axhline(y=1.0, color='blue', linestyle='--', alpha=0.7, label='Perfect Fairness (1.0)')\n",
    "ax1.set_ylabel('Disparate Impact')\n",
    "ax1.set_title('Gender Fairness: Disparate Impact')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, di_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.3f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Performance vs Fairness Trade-off\n",
    "ax2.scatter([accuracy_orig], [di_orig], s=100, color='red', alpha=0.7, label='Original')\n",
    "ax2.scatter([accuracy_reweighed], [di_reweighed], s=100, color='green', alpha=0.7, label='Bias-Mitigated')\n",
    "ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Fairness Threshold')\n",
    "ax2.set_xlabel('Accuracy')\n",
    "ax2.set_ylabel('Disparate Impact')\n",
    "ax2.set_title('Performance vs Fairness Trade-off')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrows showing improvement\n",
    "ax2.annotate('', xy=(accuracy_reweighed, di_reweighed), xytext=(accuracy_orig, di_orig),\n",
    "             arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "# 3. Group-specific Performance\n",
    "tpr_female_orig = cm_orig.true_positive_rate(privileged=False)\n",
    "tpr_male_orig = cm_orig.true_positive_rate(privileged=True)\n",
    "tpr_female_reweighed = cm_reweighed.true_positive_rate(privileged=False)\n",
    "tpr_male_reweighed = cm_reweighed.true_positive_rate(privileged=True)\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x - width/2, [tpr_female_orig, tpr_male_orig], width, \n",
    "        label='Original', alpha=0.7, color='#ff7f7f')\n",
    "ax3.bar(x + width/2, [tpr_female_reweighed, tpr_male_reweighed], width,\n",
    "        label='Bias-Mitigated', alpha=0.7, color='#7f7fff')\n",
    "\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title('High-Income Prediction Rate by Gender')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Female', 'Male'])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Multiple Fairness Metrics Comparison\n",
    "metrics_names = ['Disp Impact', 'Stat Parity', 'Equal Opp', 'Avg Odds']\n",
    "orig_values = [cm_orig.disparate_impact(), \n",
    "               abs(cm_orig.statistical_parity_difference()),\n",
    "               abs(cm_orig.equal_opportunity_difference()), \n",
    "               abs(cm_orig.average_odds_difference())]\n",
    "reweighed_values = [cm_reweighed.disparate_impact(),\n",
    "                   abs(cm_reweighed.statistical_parity_difference()),\n",
    "                   abs(cm_reweighed.equal_opportunity_difference()),\n",
    "                   abs(cm_reweighed.average_odds_difference())]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "ax4.bar(x - width/2, orig_values, width, label='Original', alpha=0.7, color='#ff7f7f')\n",
    "ax4.bar(x + width/2, reweighed_values, width, label='Bias-Mitigated', alpha=0.7, color='#7f7fff')\n",
    "ax4.set_xlabel('Fairness Metrics')\n",
    "ax4.set_ylabel('Metric Values')\n",
    "ax4.set_title('Multi-Metric Fairness Comparison')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics_names, fontsize=9)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90376973-5717-4a07-b71e-1ed25c383d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: EXECUTIVE SUMMARY AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Calculate improvements\n",
    "di_improvement = di_reweighed - di_orig\n",
    "di_improvement_pct = (di_improvement / di_orig) * 100\n",
    "\n",
    "print(f\" BIAS MITIGATION RESULTS:\")\n",
    "print(f\"• Disparate Impact: {di_orig:.3f} → {di_reweighed:.3f}\")\n",
    "print(f\"• Absolute Improvement: {di_improvement:+.3f}\")\n",
    "print(f\"• Relative Improvement: {di_improvement_pct:+.1f}%\")\n",
    "print(f\"• Accuracy Impact: {performance_change:+.1%}\")\n",
    "\n",
    "# Business recommendation\n",
    "if di_reweighed >= 0.8 and abs(performance_change) < 0.02:\n",
    "    recommendation = \" DEPLOY BIAS-MITIGATED MODEL\"\n",
    "    reason = \"Achieves fairness threshold with minimal performance impact\"\n",
    "elif di_reweighed >= 0.8:\n",
    "    recommendation = \" DEPLOY WITH MONITORING\"\n",
    "    reason = \"Achieves fairness but monitor performance closely\"\n",
    "elif di_improvement > 0.1:\n",
    "    recommendation = \" CONTINUE DEVELOPMENT\"\n",
    "    reason = \"Good progress but needs additional bias mitigation techniques\"\n",
    "else:\n",
    "    recommendation = \" DO NOT DEPLOY - EXPLORE ALTERNATIVES\"\n",
    "    reason = \"Insufficient bias reduction achieved\"\n",
    "\n",
    "print(f\"\\n BUSINESS RECOMMENDATION: {recommendation}\")\n",
    "print(f\" Rationale: {reason}\")\n",
    "\n",
    "print(f\"\\n NEXT STEPS:\")\n",
    "if \"DEPLOY\" in recommendation:\n",
    "    print(\"• Implement model in production environment\")\n",
    "    print(\"• Set up ongoing fairness monitoring\")\n",
    "    print(\"• Establish regular bias auditing schedule\")\n",
    "    print(\"• Document bias mitigation approach for compliance\")\n",
    "else:\n",
    "    print(\"• Try additional AIF360 algorithms (e.g., post-processing methods)\")\n",
    "    print(\"• Consider ensemble approaches combining multiple bias mitigation techniques\")\n",
    "    print(\"• Evaluate alternative fairness metrics and thresholds\")\n",
    "    print(\"• Reassess feature engineering and data collection practices\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f49cc-cb9d-4c1d-acf6-92abe57a5c7c",
   "metadata": {},
   "source": [
    "** AIF360 CAPABILITIES DEMONSTRATED: **\n",
    "- Comprehensive bias detection across multiple fairness definitions\n",
    "- Pre-processing bias mitigation with Reweighing algorithm\n",
    "- Performance vs fairness trade-off analysis\n",
    "- Group-specific prediction analysis\n",
    "- Executive-friendly reporting and visualization\n",
    "- Regulatory compliance assessment framework\n",
    "\n",
    "** ADDITIONAL AIF360 TOOLS AVAILABLE: **\n",
    "- Pre-processing: Disparate Impact Remover, Learning Fair Representations\n",
    "- In-processing: Adversarial Debiasing, Prejudice Remover  \n",
    "- Post-processing: Equalized Odds, Calibrated Equalized Odds\n",
    "- Explainability: Bias detection explanations and interpretations\n",
    "\n",
    "- Analysis Complete! This framework can be adapted for various\n",
    "  income prediction scenarios while ensuring gender fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eecf04-bd6a-4dc0-b0b6-fd7fa86495b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b44358-976f-4ed2-b631-5f38385cd1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28127c9e-184b-404b-86ca-dc82c39ff2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
