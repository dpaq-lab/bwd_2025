{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659fb9a-f11f-479d-8e5d-05a9b3952b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Binder session alive during workshop\n",
    "import time\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "def keep_alive():\n",
    "    display(Javascript('setInterval(function(){ console.log(\"keeping alive\"); }, 300000);'))\n",
    "    \n",
    "keep_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040f5f3-7850-493c-a867-e09423b6bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIF360 Bias Detection & Mitigation Workshop\n",
    "# For Directors and Managers: Understanding Ethical AI Toolboxes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# AIF360 imports\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738dde3d-4734-465e-8b72-f18e82b75421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import advanced features (may not be available in all environments)\n",
    "try:\n",
    "    from aif360.explainers import MetricTextExplainer\n",
    "    EXPLAINER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\" MetricTextExplainer not available in this environment\")\n",
    "    EXPLAINER_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import aif360.sklearn.metrics as aif_sklearn_metrics\n",
    "    SKLEARN_METRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_METRICS_AVAILABLE = False\n",
    "\n",
    "print(\" Welcome to the AIF360 Example!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b2517-524a-44f4-a3b7-d16e65a68b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: BUSINESS CONTEXT - WHY THIS MATTERS TO LEADERSHIP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n BUSINESS CONTEXT FOR LEADERSHIP\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "KEY QUESTIONS FOR DIRECTORS/MANAGERS:\n",
    "• What are the financial/legal risks of biased AI systems?\n",
    "• How do we measure and communicate bias to stakeholders?\n",
    "• What tools can our data science teams use to mitigate bias?\n",
    "• How do we balance fairness with business performance?\n",
    "\n",
    "This notebook demonstrates AIF360's capabilities using a hiring scenario.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a42b3-5c38-4d8b-a6d0-1c7bbcb0e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n DATA LOADING AND PREPARATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Define column names for Adult dataset\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "               'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "               'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Load the data files from your repo\n",
    "print(\" Loading Adult dataset from local files...\")\n",
    "df_train = pd.read_csv('data/adult/adult.data', names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "df_test = pd.read_csv('data/adult/adult.test', names=column_names, na_values=' ?', skipinitialspace=True, skiprows=1)\n",
    "\n",
    "# Combine datasets\n",
    "df_raw = pd.concat([df_train, df_test], ignore_index=True)\n",
    "print(f\" Loaded {len(df_raw):,} records\")\n",
    "\n",
    "# Clean the data\n",
    "print(\" Cleaning and preparing data...\")\n",
    "\n",
    "# Remove missing values\n",
    "df_clean = df_raw.dropna()\n",
    "print(f\"After removing missing values: {len(df_clean):,} records\")\n",
    "\n",
    "# Clean income column (remove periods from test set)\n",
    "df_clean['income'] = df_clean['income'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Create binary target variable (0: <=50K, 1: >50K)\n",
    "df_clean['target'] = (df_clean['income'] == '>50K').astype(int)\n",
    "\n",
    "# Create binary sex variable (0: Female, 1: Male)\n",
    "df_clean['sex_binary'] = (df_clean['sex'] == 'Male').astype(int)\n",
    "\n",
    "# Encode categorical variables for AIF360 compatibility\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                   'relationship', 'race', 'native-country']\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_processed = df_clean.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "\n",
    "# Create the final cleaned dataset that will be used throughout\n",
    "df_final = df_processed[['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                        'marital-status', 'occupation', 'relationship', 'race', \n",
    "                        'sex_binary', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                        'native-country', 'target']].copy()\n",
    "\n",
    "print(\" Data preparation complete!\")\n",
    "print(f\"Final dataset: {df_final.shape[0]} rows × {df_final.shape[1]} columns\")\n",
    "print(\"\\n Demographics in final dataset:\")\n",
    "print(f\"Female (0): {(df_final['sex_binary'] == 0).sum():,} ({(df_final['sex_binary'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Male (1): {(df_final['sex_binary'] == 1).sum():,} ({(df_final['sex_binary'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"High income (>50K): {df_final['target'].sum():,} ({df_final['target'].mean()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f8b39-36ae-4866-a4ab-2d893004fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: CREATE AIF360 DATASET AND INITIAL BIAS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n SECTION 3: INITIAL BIAS MEASUREMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create AIF360 StandardDataset\n",
    "dataset = StandardDataset(\n",
    "    df=df_final,\n",
    "    label_name='target',\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=['sex_binary'],\n",
    "    privileged_classes=[[1]],  # Male = 1\n",
    "    categorical_features=['workclass', 'education', 'marital-status', 'occupation', \n",
    "                         'relationship', 'race', 'native-country']\n",
    ")\n",
    "\n",
    "print(\" Created AIF360 dataset structure\")\n",
    "\n",
    "# Split the data\n",
    "train_dataset, test_dataset = dataset.split([0.7], shuffle=True, seed=123)\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "privileged_groups = [{'sex_binary': 1}]    # Male\n",
    "unprivileged_groups = [{'sex_binary': 0}]  # Female\n",
    "\n",
    "# Calculate bias metrics on training data\n",
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "    train_dataset,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\" BIAS METRICS IN ORIGINAL TRAINING DATA:\")\n",
    "disparate_impact = metric_orig_train.disparate_impact()\n",
    "statistical_parity_diff = metric_orig_train.statistical_parity_difference()\n",
    "\n",
    "print(f\"Disparate Impact: {disparate_impact:.3f}\")\n",
    "print(f\"Statistical Parity Difference: {statistical_parity_diff:.3f}\")\n",
    "\n",
    "print(\"\\n WHAT THESE NUMBERS MEAN FOR LEADERSHIP:\")\n",
    "print(\"• Disparate Impact < 0.8 = Potential legal risk (80% rule)\")\n",
    "print(\"• Statistical Parity Difference > 0.1 = Significant bias concern\")\n",
    "\n",
    "# Risk assessment\n",
    "risk_level = \" HIGH RISK\" if disparate_impact < 0.8 else \" MEDIUM RISK\" if disparate_impact < 0.9 else \" LOW RISK\"\n",
    "print(f\"• Current Risk Level: {risk_level}\")\n",
    "\n",
    "# Visualize the bias\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Get training data as DataFrame for visualization\n",
    "train_df = train_dataset.convert_to_dataframe()[0]\n",
    "\n",
    "# Outcome rates by gender\n",
    "outcome_by_gender = train_df.groupby('sex_binary')['target'].mean()\n",
    "ax1.bar(['Female', 'Male'], outcome_by_gender, color=['#ff7f7f', '#7f7fff'])\n",
    "ax1.set_title('High-Income Rate by Gender\\n(Training Data)')\n",
    "ax1.set_ylabel('Rate of High-Income (>50K)')\n",
    "ax1.set_ylim(0, 0.5)\n",
    "for i, v in enumerate(outcome_by_gender):\n",
    "    ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Risk dashboard\n",
    "ax2.text(0.5, 0.7, f'Disparate Impact: {disparate_impact:.3f}', \n",
    "         ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax2.text(0.5, 0.5, risk_level, ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "ax2.text(0.5, 0.3, 'Legal threshold: 0.8', ha='center', va='center', fontsize=12)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('Legal Risk Assessment')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e494c2e-897a-409c-9d42-69b31180f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: AIF360 BIAS MITIGATION - REWEIGHING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n SECTION 4: AIF360 BIAS MITIGATION\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "print(\"AIF360 offers multiple bias mitigation techniques:\")\n",
    "print(\"• PRE-processing: Fix bias in training data (Reweighing)\")\n",
    "print(\"• IN-processing: Build fairness into model training\")  \n",
    "print(\"• POST-processing: Adjust model outputs for fairness\")\n",
    "print(\"\\nDemonstrating PRE-processing with Reweighing...\")\n",
    "\n",
    "# Apply Reweighing algorithm\n",
    "np.random.seed(123)\n",
    "reweighing = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Transform training data\n",
    "train_dataset_reweighed = reweighing.fit_transform(train_dataset)\n",
    "print(\"Applied Reweighing to training data\")\n",
    "\n",
    "# Check bias metrics after reweighing\n",
    "metric_reweighed = BinaryLabelDatasetMetric(\n",
    "    train_dataset_reweighed,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"\\n BIAS METRICS AFTER REWEIGHING:\")\n",
    "disparate_impact_new = metric_reweighed.disparate_impact()\n",
    "statistical_parity_diff_new = metric_reweighed.statistical_parity_difference()\n",
    "\n",
    "print(f\"Original Disparate Impact: {disparate_impact:.3f}\")\n",
    "print(f\"After Reweighing: {disparate_impact_new:.3f}\")\n",
    "improvement = ((disparate_impact_new - disparate_impact) / abs(disparate_impact) * 100)\n",
    "print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "print(f\"\\nOriginal Statistical Parity Diff: {statistical_parity_diff:.3f}\")\n",
    "print(f\"After Reweighing: {statistical_parity_diff_new:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f547225-c330-4789-b090-f46d32e9158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: MODEL TRAINING AND BUSINESS IMPACT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n SECTION 5: BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"Training models to compare business performance...\")\n",
    "\n",
    "# Prepare data for sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Original model (without bias mitigation)\n",
    "X_train_orig = scaler.fit_transform(train_dataset.features)\n",
    "y_train_orig = train_dataset.labels.ravel()\n",
    "\n",
    "model_orig = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)\n",
    "model_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "# Bias-mitigated model (with reweighing)\n",
    "X_train_reweighed = scaler.fit_transform(train_dataset_reweighed.features)\n",
    "y_train_reweighed = train_dataset_reweighed.labels.ravel()\n",
    "sample_weights = train_dataset_reweighed.instance_weights\n",
    "\n",
    "model_reweighed = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)\n",
    "model_reweighed.fit(X_train_reweighed, y_train_reweighed, sample_weight=sample_weights)\n",
    "\n",
    "# Test both models\n",
    "X_test = scaler.transform(test_dataset.features)\n",
    "y_test = test_dataset.labels.ravel()\n",
    "\n",
    "y_pred_orig = model_orig.predict(X_test)\n",
    "y_pred_reweighed = model_reweighed.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_orig = accuracy_score(y_test, y_pred_orig)\n",
    "accuracy_reweighed = accuracy_score(y_test, y_pred_reweighed)\n",
    "\n",
    "print(\" EXECUTIVE DASHBOARD: MODEL COMPARISON\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "print(\" BUSINESS PERFORMANCE:\")\n",
    "print(f\"Original Model Accuracy: {accuracy_orig:.1%}\")\n",
    "print(f\"Reweighed Model Accuracy: {accuracy_reweighed:.1%}\")\n",
    "print(f\"Performance Trade-off: {accuracy_reweighed - accuracy_orig:+.1%}\")\n",
    "\n",
    "# Calculate fairness metrics for predictions\n",
    "test_pred_orig = test_dataset.copy()\n",
    "test_pred_orig.labels = y_pred_orig\n",
    "metric_test_orig = BinaryLabelDatasetMetric(\n",
    "    test_pred_orig, \n",
    "    unprivileged_groups=unprivileged_groups, \n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "test_pred_reweighed = test_dataset.copy() \n",
    "test_pred_reweighed.labels = y_pred_reweighed\n",
    "metric_test_reweighed = BinaryLabelDatasetMetric(\n",
    "    test_pred_reweighed,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"\\n FAIRNESS METRICS ON TEST PREDICTIONS:\")\n",
    "di_orig = metric_test_orig.disparate_impact()\n",
    "di_reweighed = metric_test_reweighed.disparate_impact()\n",
    "\n",
    "print(f\"Original Model Disparate Impact: {di_orig:.3f}\")\n",
    "print(f\"Reweighed Model Disparate Impact: {di_reweighed:.3f}\")\n",
    "\n",
    "# Risk levels\n",
    "orig_risk = \" HIGH\" if di_orig < 0.8 else \" MEDIUM\" if di_orig < 0.9 else \" LOW\"\n",
    "reweighed_risk = \" HIGH\" if di_reweighed < 0.8 else \" MEDIUM\" if di_reweighed < 0.9 else \" LOW\"\n",
    "\n",
    "print(f\"\\n LEGAL RISK ASSESSMENT:\")\n",
    "print(f\"Original Model: {orig_risk}\")  \n",
    "print(f\"Reweighed Model: {reweighed_risk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c12a2-0260-40c9-89ab-826256d9cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5B: AIF360'S COMPREHENSIVE FAIRNESS METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n SECTION 5B: AIF360'S MULTI-METRIC FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 54)\n",
    "print(\"AIF360's key advantage: Multiple fairness definitions in one tool...\")\n",
    "\n",
    "# Create ClassificationMetric for comprehensive analysis (AIF360-specific)\n",
    "cm_orig = ClassificationMetric(\n",
    "    test_dataset, test_pred_orig,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "cm_reweighed = ClassificationMetric(\n",
    "    test_dataset, test_pred_reweighed, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# AIF360's comprehensive metrics (this is what makes it unique)\n",
    "print(\"\\n AIF360'S FAIRNESS METRICS COMPARISON:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "fairness_metrics = [\n",
    "    (\"Disparate Impact\", cm_orig.disparate_impact(), cm_reweighed.disparate_impact(), \">0.8 (legal threshold)\"),\n",
    "    (\"Statistical Parity Difference\", cm_orig.statistical_parity_difference(), cm_reweighed.statistical_parity_difference(), \"~0.0 (equal rates)\"),\n",
    "    (\"Equal Opportunity Difference\", cm_orig.equal_opportunity_difference(), cm_reweighed.equal_opportunity_difference(), \"~0.0 (equal TPR)\"),\n",
    "    (\"Average Odds Difference\", cm_orig.average_odds_difference(), cm_reweighed.average_odds_difference(), \"~0.0 (balanced errors)\"),\n",
    "    (\"Theil Index\", cm_orig.theil_index(), cm_reweighed.theil_index(), \"Lower is better\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Metric':<30} {'Original':<10} {'Reweighed':<10} {'Target':<20}\")\n",
    "print(\"-\" * 75)\n",
    "for name, orig_val, reweighed_val, target in fairness_metrics:\n",
    "    print(f\"{name:<30} {orig_val:<10.3f} {reweighed_val:<10.3f} {target:<20}\")\n",
    "\n",
    "print(f\"\\nAccuracy                       {accuracy_orig:<10.3f} {accuracy_reweighed:<10.3f} {'Higher is better':<20}\")\n",
    "\n",
    "print(\"\\n WHY THESE AIF360 METRICS MATTER TO LEADERSHIP:\")\n",
    "print(\"• Disparate Impact: Legal compliance metric (EEOC 80% rule)\")\n",
    "print(\"• Statistical Parity: Equal selection rates across groups\") \n",
    "print(\"• Equal Opportunity: Fair treatment of qualified candidates\")\n",
    "print(\"• Average Odds: Balanced across all prediction types\")\n",
    "print(\"• Theil Index: Overall systemic inequality measure\")\n",
    "\n",
    "# AIF360's group-specific metrics (another unique feature)\n",
    "print(\"\\n AIF360'S GROUP-SPECIFIC ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"ORIGINAL MODEL - Group Performance:\")\n",
    "print(f\"Female True Positive Rate: {cm_orig.true_positive_rate(privileged=False):.3f}\")\n",
    "print(f\"Male True Positive Rate: {cm_orig.true_positive_rate(privileged=True):.3f}\")\n",
    "print(f\"Female False Positive Rate: {cm_orig.false_positive_rate(privileged=False):.3f}\")\n",
    "print(f\"Male False Positive Rate: {cm_orig.false_positive_rate(privileged=True):.3f}\")\n",
    "\n",
    "print(\"\\nREWEIGHED MODEL - Group Performance:\")\n",
    "print(f\"Female True Positive Rate: {cm_reweighed.true_positive_rate(privileged=False):.3f}\")\n",
    "print(f\"Male True Positive Rate: {cm_reweighed.true_positive_rate(privileged=True):.3f}\")\n",
    "print(f\"Female False Positive Rate: {cm_reweighed.false_positive_rate(privileged=False):.3f}\")\n",
    "print(f\"Male False Positive Rate: {cm_reweighed.false_positive_rate(privileged=True):.3f}\")\n",
    "\n",
    "# Simple AIF360-focused visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# AIF360 Disparate Impact visualization\n",
    "models = ['Original', 'Reweighed']\n",
    "di_values = [cm_orig.disparate_impact(), cm_reweighed.disparate_impact()]\n",
    "colors = ['red' if x < 0.8 else 'orange' if x < 0.9 else 'green' for x in di_values]\n",
    "\n",
    "bars = ax1.bar(models, di_values, color=colors, alpha=0.7)\n",
    "ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Legal Threshold (0.8)')\n",
    "ax1.set_ylabel('Disparate Impact')\n",
    "ax1.set_title('AIF360: Disparate Impact Analysis')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, di_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.3f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# AIF360 Multi-metric summary\n",
    "metrics_names = ['Disp Impact', 'Stat Parity', 'Equal Opp', 'Avg Odds']\n",
    "orig_abs_values = [cm_orig.disparate_impact(), \n",
    "                   abs(cm_orig.statistical_parity_difference()),\n",
    "                   abs(cm_orig.equal_opportunity_difference()), \n",
    "                   abs(cm_orig.average_odds_difference())]\n",
    "reweighed_abs_values = [cm_reweighed.disparate_impact(),\n",
    "                       abs(cm_reweighed.statistical_parity_difference()),\n",
    "                       abs(cm_reweighed.equal_opportunity_difference()),\n",
    "                       abs(cm_reweighed.average_odds_difference())]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, orig_abs_values, width, label='Original', alpha=0.7, color='#ff7f7f')\n",
    "ax2.bar(x + width/2, reweighed_abs_values, width, label='Reweighed', alpha=0.7, color='#7f7fff')\n",
    "ax2.set_xlabel('AIF360 Fairness Metrics')\n",
    "ax2.set_ylabel('Metric Values')\n",
    "ax2.set_title('AIF360: Multi-Metric Fairness Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics_names, fontsize=9)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n ADDITIONAL AIF360 BIAS MITIGATION ALGORITHMS:\")\n",
    "print(\"(Available in your data science toolkit)\")\n",
    "print(\"• PRE-processing: Reweighing, Disparate Impact Remover, Learning Fair Representations\")\n",
    "print(\"• IN-processing: Adversarial Debiasing, Fair Adversarial Networks, Prejudice Remover\")  \n",
    "print(\"• POST-processing: Equalized Odds, Calibrated Equalized Odds, Reject Option Classification\")\n",
    "\n",
    "# AIF360's Metric Text Explainer\n",
    "print(\"\\n AIF360'S AUTOMATED FAIRNESS EXPLANATION:\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "if EXPLAINER_AVAILABLE:\n",
    "    try:\n",
    "        # Correct usage based on the example you provided\n",
    "        explainer = MetricTextExplainer(cm_orig)\n",
    "        explainer_reweighed = MetricTextExplainer(cm_reweighed)\n",
    "        \n",
    "        print(\" Original Model Assessment:\")\n",
    "        print(f\"Disparate Impact: {cm_orig.disparate_impact():.3f}\")\n",
    "        print(explainer.disparate_impact())\n",
    "        \n",
    "        print(f\"\\nStatistical Parity Difference: {cm_orig.statistical_parity_difference():.3f}\")\n",
    "        if hasattr(explainer, 'statistical_parity_difference'):\n",
    "            print(explainer.statistical_parity_difference())\n",
    "        \n",
    "        print(f\"\\nEqual Opportunity Difference: {cm_orig.equal_opportunity_difference():.3f}\")\n",
    "        if hasattr(explainer, 'equal_opportunity_difference'):\n",
    "            print(explainer.equal_opportunity_difference())\n",
    "        \n",
    "        print(\"\\n Reweighed Model Assessment:\")\n",
    "        print(f\"Disparate Impact: {cm_reweighed.disparate_impact():.3f}\")\n",
    "        print(explainer_reweighed.disparate_impact())\n",
    "        \n",
    "        print(f\"\\nStatistical Parity Difference: {cm_reweighed.statistical_parity_difference():.3f}\")\n",
    "        if hasattr(explainer_reweighed, 'statistical_parity_difference'):\n",
    "            print(explainer_reweighed.statistical_parity_difference())\n",
    "            \n",
    "        print(f\"\\nEqual Opportunity Difference: {cm_reweighed.equal_opportunity_difference():.3f}\")\n",
    "        if hasattr(explainer_reweighed, 'equal_opportunity_difference'):\n",
    "            print(explainer_reweighed.equal_opportunity_difference())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Explainer error: {e}\")\n",
    "        print(\" Note: MetricTextExplainer may have version-specific requirements\")\n",
    "        print(\" Fairness Assessment Summary (Enhanced Manual Analysis):\")\n",
    "        \n",
    "        # Enhanced manual analysis that mimics what the explainer would do\n",
    "        print(\"\\n DETAILED BIAS ANALYSIS:\")\n",
    "        \n",
    "        # Original model assessment\n",
    "        di_orig_text = \"significant bias\" if cm_orig.disparate_impact() < 0.8 else \"moderate bias\" if cm_orig.disparate_impact() < 0.9 else \"acceptable fairness\"\n",
    "        print(f\"\\n Original Model Analysis:\")\n",
    "        print(f\"• Disparate Impact: {cm_orig.disparate_impact():.3f} - Shows {di_orig_text}\")\n",
    "        print(f\"• Statistical Parity Difference: {cm_orig.statistical_parity_difference():.3f}\")\n",
    "        print(f\"• Equal Opportunity Difference: {cm_orig.equal_opportunity_difference():.3f}\")\n",
    "        \n",
    "        # Risk interpretation\n",
    "        if cm_orig.disparate_impact() < 0.8:\n",
    "            print(\"•  HIGH LEGAL RISK: Model falls below 80% rule threshold\")\n",
    "        elif cm_orig.disparate_impact() < 0.9:\n",
    "            print(\"•  MODERATE RISK: Model should be improved for better compliance\")\n",
    "        else:\n",
    "            print(\"• ACCEPTABLE: Model meets basic fairness thresholds\")\n",
    "        \n",
    "        # Reweighed model assessment\n",
    "        di_reweighed_text = \"significant bias\" if cm_reweighed.disparate_impact() < 0.8 else \"moderate bias\" if cm_reweighed.disparate_impact() < 0.9 else \"good fairness\"\n",
    "        print(f\"\\n Reweighed Model Analysis:\")\n",
    "        print(f\"• Disparate Impact: {cm_reweighed.disparate_impact():.3f} - Demonstrates {di_reweighed_text}\")\n",
    "        print(f\"• Statistical Parity Difference: {cm_reweighed.statistical_parity_difference():.3f}\")\n",
    "        print(f\"• Equal Opportunity Difference: {cm_reweighed.equal_opportunity_difference():.3f}\")\n",
    "        \n",
    "        # Improvement assessment\n",
    "        improvement_di = cm_reweighed.disparate_impact() - cm_orig.disparate_impact()\n",
    "        print(f\"• Disparate Impact Improvement: {improvement_di:+.3f}\")\n",
    "        \n",
    "        # Final recommendation with realistic assessment\n",
    "        print(f\"\\n BIAS MITIGATION ASSESSMENT:\")\n",
    "        if improvement_di > 0:\n",
    "            print(\" POSITIVE: Bias reduction achieved\")\n",
    "            if cm_reweighed.disparate_impact() >= 0.8:\n",
    "                print(\" SUCCESS: Now meets 80% rule threshold\")\n",
    "                print(\" RECOMMENDATION: Deploy reweighed model\")\n",
    "            else:\n",
    "                print(\" PARTIAL: Still below 80% threshold but improved\")\n",
    "                print(\" RECOMMENDATION: Deploy with additional monitoring, consider combining with other techniques\")\n",
    "        else:\n",
    "            print(\" CONCERNING: Limited or no bias reduction\")\n",
    "            print(\" RECOMMENDATION: Try different mitigation algorithms (e.g., post-processing)\")\n",
    "            \n",
    "else:\n",
    "    print(\" MetricTextExplainer not available in this environment\")\n",
    "    print(\" Comprehensive Fairness Assessment:\")\n",
    "    \n",
    "    # Detailed manual analysis\n",
    "    print(f\"\\n Original Model Analysis:\")\n",
    "    di_orig_text = \"significant bias\" if cm_orig.disparate_impact() < 0.8 else \"moderate bias\" if cm_orig.disparate_impact() < 0.9 else \"acceptable fairness\"\n",
    "    print(f\"• Shows {di_orig_text} against female candidates\")\n",
    "    print(f\"• Disparate Impact: {cm_orig.disparate_impact():.3f}\")\n",
    "    \n",
    "    print(f\"\\n Reweighed Model Analysis:\")\n",
    "    di_reweighed_text = \"significant bias\" if cm_reweighed.disparate_impact() < 0.8 else \"moderate bias\" if cm_reweighed.disparate_impact() < 0.9 else \"good fairness\"\n",
    "    print(f\"• Demonstrates {di_reweighed_text}\")  \n",
    "    print(f\"• Disparate Impact: {cm_reweighed.disparate_impact():.3f}\")\n",
    "    \n",
    "    print(f\"\\n Recommendation: {'Deploy reweighed model' if cm_reweighed.disparate_impact() > cm_orig.disparate_impact() else 'Consider additional bias mitigation'}\")\n",
    "\n",
    "# Add executive summary of results\n",
    "print(f\"\\n EXECUTIVE SUMMARY OF BIAS MITIGATION:\")\n",
    "improvement_di = cm_reweighed.disparate_impact() - cm_orig.disparate_impact()\n",
    "improvement_percent = (improvement_di / cm_orig.disparate_impact()) * 100\n",
    "\n",
    "print(f\" BIAS REDUCTION RESULTS:\")\n",
    "print(f\"• Disparate Impact: {cm_orig.disparate_impact():.3f} → {cm_reweighed.disparate_impact():.3f}\")\n",
    "print(f\"• Absolute Improvement: {improvement_di:+.3f}\")\n",
    "print(f\"• Relative Improvement: {improvement_percent:+.1f}%\")\n",
    "\n",
    "if cm_reweighed.disparate_impact() >= 0.8:\n",
    "    print(\" SUCCESS: Model now meets legal compliance threshold (0.8)\")\n",
    "    recommendation = \"DEPLOY with confidence\"\n",
    "elif improvement_di > 0.1:\n",
    "    print(\" GOOD PROGRESS: Significant improvement but still needs work\")\n",
    "    recommendation = \"DEPLOY with additional bias monitoring\"\n",
    "elif improvement_di > 0.05:\n",
    "    print(\" MODEST PROGRESS: Some improvement achieved\")\n",
    "    recommendation = \"DEPLOY with caution, plan additional mitigation\"\n",
    "else:\n",
    "    print(\" LIMITED SUCCESS: Minimal bias reduction\")\n",
    "    recommendation = \"DO NOT DEPLOY - try different techniques\"\n",
    "\n",
    "print(f\" BUSINESS RECOMMENDATION: {recommendation}\")\n",
    "\n",
    "print(\"\\n ADVANCED AIF360 CAPABILITIES DEMONSTRATED:\")\n",
    "print(\"• Multi-metric fairness evaluation (5+ fairness definitions)\")\n",
    "print(\"• Automated risk assessment and categorization\")\n",
    "print(\"• Group-wise performance analysis\")\n",
    "if EXPLAINER_AVAILABLE:\n",
    "    print(\"• Automated explanations of fairness results\")\n",
    "else:\n",
    "    print(\"• Manual fairness assessment framework\")\n",
    "print(\"• Executive-friendly visualization dashboards\")\n",
    "print(\"• Comprehensive compliance reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6dddb-7d5a-4e92-83cd-be3ffcd6e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: EXECUTIVE SUMMARY & RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EXECUTIVE SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n WHAT AIF360 PROVIDES YOUR ORGANIZATION:\")\n",
    "print(\"• Standardized bias measurement across different fairness definitions\")\n",
    "print(\"• Pre-built algorithms to reduce bias in ML models\")\n",
    "print(\"• Clear metrics that translate to legal/compliance requirements\")\n",
    "print(\"• Integration with popular ML frameworks (sklearn, etc.)\")\n",
    "\n",
    "print(\"\\n KEY DECISION POINTS FOR LEADERSHIP:\")\n",
    "print(f\"1. Performance vs. Fairness Trade-off: {accuracy_reweighed - accuracy_orig:+.1%} accuracy change\")\n",
    "print(f\"2. Legal Risk Mitigation: {orig_risk} → {reweighed_risk}\")\n",
    "print(\"3. Implementation Effort: Medium (requires data science team training)\")\n",
    "print(\"4. Ongoing Monitoring: Automated bias metrics in production\")\n",
    "\n",
    "print(\"\\n RECOMMENDED NEXT STEPS:\")\n",
    "print(\"1. Establish bias monitoring KPIs for your ML models\")\n",
    "print(\"2. Train data science teams on AIF360 implementation\")\n",
    "print(\"3. Integrate bias testing into ML model validation process\")  \n",
    "print(\"4. Create executive dashboard for ongoing fairness monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719263e3-0194-4370-8042-7560371d496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n DISCUSSION QUESTIONS:\")\n",
    "print(\"• What level of performance trade-off is acceptable for reduced bias?\")\n",
    "print(\"• How should we communicate fairness metrics to business stakeholders?\")\n",
    "print(\"• What governance processes need to change to include bias testing?\")\n",
    "print(\"• How do we balance different fairness definitions for different use cases?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ce189-b003-44ae-a873-856db4eccc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
